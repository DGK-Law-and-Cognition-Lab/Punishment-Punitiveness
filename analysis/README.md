# Analysis Directory

This directory contains the analysis scripts and outputs for Punishment Study 2.1.2.

## Analysis Scripts Overview

| Script | Purpose | Language |
|--------|---------|----------|
| `Punishment_2_1_2_Final_Script.Rmd` | Main confirmatory analysis (H1, H2, H3) | R |
| `Punishment_2_1_2_Supplementary_Analyses.Rmd` | Bootstrap CIs, political moderation | R |
| `Punishment_2_1_2_Parsimony_Analysis.Rmd` | Item-level analysis of low-reliability scale | R |
| `Punishment_2_1_2_NLP_Integration.Rmd` | Integration of NLP features with quantitative data | R |
| `Punishment_212_NLP_Analysis.ipynb` | Computational text analysis pipeline | Python |

---

## R Scripts

### `r/Punishment_2_1_2_Final_Script.Rmd`

The primary analysis script implementing the pre-registered analysis plan.

**To run:**
1. Open in RStudio
2. Update the data path to your local location (line 66)
3. Knit to HTML or run chunks sequentially

**Structure:**
- Phase 0: Setup and packages
- Phase 1: Data loading and filtering
- Phase 2: Reverse coding
- Phase 3: Construct creation
- Phase 4-6: Reliability and descriptives
- Phase 7: H1 testing (correlations)
- Phase 8: H2 testing (Steiger's Z)
- Phase 9: Exploratory analyses (EFA, CFA, demographics)
- Phase 10: Multiple regression
- Phase 11: Output generation

---

### `r/Punishment_2_1_2_Supplementary_Analyses.Rmd`

Additional analyses supporting the main findings.

**Contents:**
- Section 1: Correlation comparisons across punitiveness measures
- Section 2: Bootstrap confidence intervals for Steiger's Z (H2)
- Section 3: Political orientation × correlate interaction tests
- Section 4: Simple slopes analysis
- Section 5: Correlations by political group

**Requires:** `punishment_212_cleaned_data.csv` (generated by Final Script)

---

### `r/Punishment_2_1_2_Parsimony_Analysis.Rmd`

Deep dive into the parsimony scale (α = .48).

**Contents:**
- Item-level descriptives and distributions
- Cross-tabulation of items
- Separate correlations for each item
- H2 test for each item separately
- Recommendations for handling low reliability

**Requires:** `punishment_212_cleaned_data.csv`

---

### `r/Punishment_2_1_2_NLP_Integration.Rmd`

Integration of NLP features with psychological measures.

**Contents:**
- Section 1: Facade detection correlations (text × dark psychology)
- Section 2: Group comparisons (high vs. low hostile aggression)
- Section 3: Incremental validity (text predicting punitiveness beyond psych measures)
- Section 4: Text predicting sentencing behavior
- Section 5: Collective facade evidence
- Section 6: Vignette-specific analyses

**Requires:** `punishment_212_nlp_features.csv` (generated by Python notebook)

---

## Python Script

### `python/Punishment_212_NLP_Analysis.ipynb`

Comprehensive NLP pipeline for analyzing open-ended justification responses.

**To run:**
1. Upload to Google Colab
2. Set Runtime → GPU (recommended)
3. Upload `punishment_212_cleaned_data.csv` when prompted
4. Run all cells (~15-20 minutes)
5. Download output files

**Structure:**
- Section 0: Setup and package installation
- Section 1: Text preprocessing
- Section 2: Dictionary-based analysis (VADER, Empath, custom)
- Section 3: Zero-shot classification (BART-large-mnli)
- Section 4: Embedding-based analysis (sentence transformers)
- Section 5: Facade detection analysis
- Section 6: Word clouds
- Section 7: Export results

**Key packages:**
```python
transformers, sentence-transformers, bertopic,
vaderSentiment, empath, umap-learn, wordcloud,
scikit-learn, pandas, numpy, matplotlib, seaborn
```

**Outputs:**
- `punishment_212_nlp_features.csv` - Full dataset with ~80 NLP features
- `facade_correlation_matrix.csv` - Key facade correlations
- `bertopic_topics.csv` - Topic modeling results
- Multiple PNG visualizations

---

## Output Files

### Tables (`output/tables/`)

#### Main Analysis
| File | Description |
|------|-------------|
| `H1_punitiveness_correlations.csv` | Primary hypothesis test results with FDR correction |
| `H1_item_level_correlations.csv` | Item-level correlations (exploratory) |
| `reliability_alphas.csv` | Cronbach's α for all scales |
| `descriptive_statistics.csv` | Means, SDs, ranges for all variables |
| `correlation_table_construct_level.csv` | Full correlation matrix |
| `correlation_table_intercorrelations.csv` | Correlate intercorrelations (H3) |
| `vignette_correlation_stability.csv` | Cross-vignette stability |
| `pvalues_construct_level.csv` | P-values for correlation matrix |

#### NLP Analysis
| File | Description |
|------|-------------|
| `nlp_facade_correlations.csv` | Text features × psychological measures |
| `nlp_group_comparisons.csv` | High vs. low hostile aggression text differences |
| `nlp_sentence_correlations.csv` | Text features predicting sentencing |
| `nlp_vignette_correlations.csv` | Facade patterns by vignette |
| `facade_correlation_matrix.csv` | Key facade detection correlations |
| `bertopic_topics.csv` | BERTopic topic modeling results |

### Figures (`output/figures/`)

#### Main Analysis
| File | Description |
|------|-------------|
| `heatmap_construct_level.png` | Punitiveness × 16 constructs |
| `heatmap_cluster_level.png` | Punitiveness × 4 clusters |
| `heatmap_intercorrelations.png` | Correlate intercorrelations |
| `histogram_sentence.png` | Overall sentence distribution |
| `histogram_sentence_by_vignette.png` | Sentences by condition |
| `efa_scree_plot.png` | Factor analysis scree plot |

#### NLP Analysis
| File | Description |
|------|-------------|
| `nlp_correlation_heatmap.png` | Text features × psychological measures heatmap |
| `facade_scatter_plot.png` | Hostile aggression vs. prosocial language |
| `facade_visualizations.png` | UMAP plots and distribution panels |
| `wordcloud_all.png` | Word cloud for all responses |
| `wordcloud_high_hostile.png` | Word cloud for high hostile group |
| `wordcloud_low_hostile.png` | Word cloud for low hostile group |

---

## Required Packages

### R
```r
required_packages <- c(
  "tidyverse", "psych", "corrplot", "Hmisc", "cocor",
  "lme4", "lmerTest", "car", "effectsize", "ggcorrplot",
  "RColorBrewer", "knitr", "broom", "scales", "performance", 
  "lavaan", "boot", "interactions"
)

install.packages(required_packages)
```

### Python (Colab)
```python
# Installed automatically in notebook
!pip install transformers sentence-transformers bertopic
!pip install vaderSentiment empath umap-learn wordcloud
```

---

## Quick Results Access

### R: Load Key Results
```r
# Main hypothesis results
h1_results <- read.csv("output/tables/H1_punitiveness_correlations.csv")
reliability <- read.csv("output/tables/reliability_alphas.csv")
descriptives <- read.csv("output/tables/descriptive_statistics.csv")

# View top correlates
h1_results[order(-h1_results$r), c("Construct", "r", "sig_fdr")]

# NLP results
facade_cors <- read.csv("output/tables/nlp_facade_correlations.csv")
sentence_cors <- read.csv("output/tables/nlp_sentence_correlations.csv")

# Key facade test
facade_cors[facade_cors$Text_Feature == "sim_prosocial_mean" & 
            facade_cors$Psych_Measure == "hostile_agg", ]
```

### Python: Load NLP Features
```python
import pandas as pd

# Full dataset with NLP features
df = pd.read_csv("../data/processed/punishment_212_nlp_features.csv")

# Check key correlations
print(df[['sim_prosocial_mean', 'hostile_agg', 'punitiveness_agg']].corr())
```

---

## Analysis Workflow

```
1. Run Final_Script.Rmd
   └── Generates: punishment_212_cleaned_data.csv
                  H1_punitiveness_correlations.csv
                  reliability_alphas.csv
                  (all main outputs)
   
2. Run Supplementary_Analyses.Rmd
   └── Requires: punishment_212_cleaned_data.csv
   └── Generates: bootstrap_ci_results.csv
                  political_interaction_tests.csv
   
3. Run Parsimony_Analysis.Rmd (optional)
   └── Requires: punishment_212_cleaned_data.csv
   └── Generates: parsimony_items_correlations.csv
   
4. Run Punishment_212_NLP_Analysis.ipynb (Python/Colab)
   └── Requires: punishment_212_cleaned_data.csv
   └── Generates: punishment_212_nlp_features.csv
                  facade_correlation_matrix.csv
                  bertopic_topics.csv
                  (all NLP figures)
   
5. Run NLP_Integration.Rmd
   └── Requires: punishment_212_nlp_features.csv
   └── Generates: nlp_facade_correlations.csv
                  nlp_sentence_correlations.csv
                  nlp_correlation_heatmap.png
```

---

## Key Findings Summary

### Main Analysis
- **H1 Supported:** All 16 correlates positively associated with punitiveness
- **H2 Supported:** Hostile Aggression (r = .61) > Crime Concerns (r = .34), Z = 7.89, p < .001
- **H3 Supported:** 100% of correlate intercorrelations positive

### NLP Analysis
- **No individual facade:** Prosocial language × hostile aggression: r = -.04 (ns)
- **Language reflects psychology:** Dark language × punitiveness: r = .29***
- **Collective facade:** 67.5% of responses semantically closer to revenge than deterrence
- **Text predicts behavior:** Rehabilitation language × sentence: r = -.45***
